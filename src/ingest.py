# ë¬¸ì„œ ë¡œë“œ + ì²­í¬ + ì„ë² ë”© + ë²¡í„°DB ì €ì¥

import os
import unicodedata
from typing import List, Dict, Any
from dotenv import load_dotenv

from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
import chromadb

load_dotenv()

# ============================================
# ìƒìˆ˜ ì„¤ì •
# ============================================
CHROMA_BASE_DIR = "vector_store"

# GPU/CPU ìë™ ê°ì§€
import torch
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

EMBEDDING_MODEL_KWARGS = {
    'device': DEVICE
}

EMBEDDING_ENCODE_KWARGS = {
    'normalize_embeddings': True
}


# ============================================
# 1. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ============================================

def normalize_filename(filename: str) -> str:
    """
    íŒŒì¼ëª…ì„ NFCë¡œ ì •ê·œí™” (Linux/Docker í‘œì¤€)
    macOSì˜ NFD í˜•íƒœë„ NFCë¡œ í†µì¼í•˜ì—¬ ì²˜ë¦¬
    """
    return unicodedata.normalize('NFC', filename)


def extract_candidate_name(filename: str) -> str:
    """
    íŒŒì¼ëª…ì—ì„œ ì§€ì›ì ì´ë¦„ ì¶”ì¶œ (ì •ê·œí™” ì ìš©)
    ì˜ˆ: "ë°•ê´‘ì§„_CV.pdf" -> "ë°•ê´‘ì§„"
    """
    basename = os.path.basename(filename)
    # ì •ê·œí™” ì ìš©
    basename = normalize_filename(basename)
    
    if '_CV.pdf' in basename:
        candidate = basename.replace('_CV.pdf', '')
    else:
        # ê·œì¹™ì— ë§ì§€ ì•Šìœ¼ë©´ .pdf ì œê±°
        candidate = basename.replace('.pdf', '')
    
    # ë°˜í™˜ ì‹œì—ë„ ì •ê·œí™”
    return normalize_filename(candidate)


def get_chroma_dir() -> str:
    """
    ë‹¨ì¼ ë²¡í„° DB ê²½ë¡œ ë°˜í™˜
    ì˜ˆ: "vector_store/chroma_db"
    """
    return os.path.join(CHROMA_BASE_DIR, "chroma_db")


def get_candidates_dir() -> str:
    """
    ì§€ì›ì ì¸ë±ìŠ¤ìš© ChromaDB ê²½ë¡œ ë°˜í™˜
    ì˜ˆ: "vector_store/candidates_index"
    """
    return os.path.join(CHROMA_BASE_DIR, "candidates_index")


def get_candidates_collection():
    """
    ì§€ì›ì ëª©ë¡ì„ ê´€ë¦¬í•˜ëŠ” ChromaDB ì»¬ë ‰ì…˜ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    (ì§€ì›ìë¥¼ êµ¬ë³„í•˜ê¸° ìœ„í•œ ê°€ë²¼ìš´ ì¸ë±ìŠ¤)
    """
    candidates_dir = get_candidates_dir()
    os.makedirs(candidates_dir, exist_ok=True)
    client = chromadb.PersistentClient(path=candidates_dir)
    collection = client.get_or_create_collection("candidates_index")
    return collection


# ============================================
# 2. ë¬¸ì„œ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
# ============================================

def load_documents_from_paths(file_paths: List[str], original_filenames: List[str] = None) -> Dict[str, List[Any]]:
    """
    ì—¬ëŸ¬ PDF íŒŒì¼ ê²½ë¡œë¥¼ ë°›ì•„ì„œ ì§€ì›ìë³„ë¡œ ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        file_paths: PDF íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        original_filenames: ì›ë³¸ íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸ (íŒŒì¼ëª… ê¹¨ì§ ë°©ì§€ìš©)
        
    Returns:
        ì§€ì›ìë³„ ë¬¸ì„œ ë”•ì…”ë„ˆë¦¬ {ì§€ì›ìëª…: [ë¬¸ì„œë“¤]}
    """
    docs_by_candidate = {}
    
    for idx, file_path in enumerate(file_paths):
        if not os.path.exists(file_path):
            print(f"âš ï¸ ê²½ê³ : {file_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
            
        try:
            # ì›ë³¸ íŒŒì¼ëª… ì‚¬ìš© (íŒŒì¼ëª… ê¹¨ì§ ë°©ì§€)
            if original_filenames and idx < len(original_filenames):
                original_name = normalize_filename(original_filenames[idx])
            else:
                original_name = normalize_filename(os.path.basename(file_path))
            
            # ì§€ì›ì ì´ë¦„ ì¶”ì¶œ
            candidate_name = extract_candidate_name(original_name)
            
            loader = PyMuPDFLoader(file_path)
            docs = loader.load()
            
            # ë©”íƒ€ë°ì´í„°ì— ì›ë³¸ íŒŒì¼ëª…ê³¼ ì§€ì›ìëª… ì¶”ê°€
            for doc in docs:
                doc.metadata['source_file'] = original_name
                doc.metadata['candidate'] = candidate_name
            
            # ì§€ì›ìë³„ë¡œ ë¶„ë¥˜
            if candidate_name not in docs_by_candidate:
                docs_by_candidate[candidate_name] = []
            docs_by_candidate[candidate_name].extend(docs)
            
            print(f"âœ… {file_path} ë¡œë“œ ì™„ë£Œ ({len(docs)} í˜ì´ì§€) - ì§€ì›ì: {candidate_name}")
            
        except Exception as e:
            print(f"âŒ {file_path} ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            continue
    
    return docs_by_candidate


def split_documents(documents: List[Any]) -> List[Any]:
    """
    ë¬¸ì„œë¥¼ chunk ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    
    Args:
        documents: ë¶„í• í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ì²­í¬ë¡œ ë¶„í• ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1200,  # 800 â†’ 1200 (ì„¹ì…˜ ë‹¨ìœ„ ë³´ì¡´)
        chunk_overlap=300,  # 200 â†’ 300 (ê²½ê³„ ì •ë³´ ë³´ì¡´)
        length_function=len,
        separators=["\n\n", "\n", " ", ""]
    )
    chunks = text_splitter.split_documents(documents)
    return chunks


def get_embeddings():
    """
    ì„ë² ë”© ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤ (GPU ì§€ì›, í•œê¸€/ì˜ì–´ ì§€ì›)
    
    Returns:
        HuggingFaceEmbeddings ì¸ìŠ¤í„´ìŠ¤
    """
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",  # ë‹¤êµ­ì–´ ì§€ì› (í•œ/ì˜)
        model_kwargs=EMBEDDING_MODEL_KWARGS,
        encode_kwargs=EMBEDDING_ENCODE_KWARGS
    )
    return embeddings


def create_vectorstore(chunks: List[Any], persist: bool = True) -> Chroma:
    """
    ì²­í¬ë“¤ì„ ì„ë² ë”©í•˜ê³  ë‹¨ì¼ Chroma ë²¡í„°DBì— ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        chunks: ì„ë² ë”©í•  ì²­í¬ ë¦¬ìŠ¤íŠ¸ (ë©”íƒ€ë°ì´í„°ì— candidate ì •ë³´ í¬í•¨)
        persist: ë²¡í„° DBë¥¼ ë””ìŠ¤í¬ì— ì €ì¥í• ì§€ ì—¬ë¶€
        
    Returns:
        Chroma ë²¡í„°ìŠ¤í† ì–´ ì¸ìŠ¤í„´ìŠ¤
    """
    embeddings = get_embeddings()
    
    chroma_dir = get_chroma_dir()
    os.makedirs(chroma_dir, exist_ok=True)
    
    # í´ë” ìƒì„± í™•ì¸
    if not os.path.exists(chroma_dir):
        raise RuntimeError(f"ë²¡í„° DB ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {chroma_dir}")
    
    vectorstore = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=chroma_dir,
    )
    
    if persist:
        vectorstore.persist()
    
    # ì €ì¥ í›„ ë‹¤ì‹œ í™•ì¸
    if not os.path.exists(chroma_dir):
        raise RuntimeError(f"ë²¡í„° DB ì €ì¥ í›„ ë””ë ‰í† ë¦¬ í™•ì¸ ì‹¤íŒ¨: {chroma_dir}")
    
    return vectorstore


def add_to_existing_vectorstore(chunks: List[Any]) -> Chroma:
    """
    ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ì— ìƒˆë¡œìš´ ì²­í¬ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
    
    Args:
        chunks: ì¶”ê°€í•  ì²­í¬ ë¦¬ìŠ¤íŠ¸ (ë©”íƒ€ë°ì´í„°ì— candidate ì •ë³´ í¬í•¨)
        
    Returns:
        ì—…ë°ì´íŠ¸ëœ Chroma ë²¡í„°ìŠ¤í† ì–´ ì¸ìŠ¤í„´ìŠ¤
    """
    embeddings = get_embeddings()
    chroma_dir = get_chroma_dir()
    
    # ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
    vectorstore = Chroma(
        embedding_function=embeddings,
        persist_directory=chroma_dir,
    )
    
    # ìƒˆ ë¬¸ì„œ ì¶”ê°€
    vectorstore.add_documents(chunks)
    vectorstore.persist()
    
    return vectorstore


# ============================================
# 3. ë©”ì¸ íŒŒì´í”„ë¼ì¸
# ============================================

def process_uploaded_documents(file_paths: List[str], original_filenames: List[str] = None) -> Dict[str, Any]:
    """
    ì—…ë¡œë“œëœ ë¬¸ì„œë“¤ì„ ì§€ì›ìë³„ë¡œ ì²˜ë¦¬í•˜ì—¬ ë²¡í„° DBì— ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        file_paths: ì²˜ë¦¬í•  PDF íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        original_filenames: ì›ë³¸ íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸ (íŒŒì¼ëª… ê¹¨ì§ ë°©ì§€ìš©)
        
    Returns:
        ì²˜ë¦¬ ê²°ê³¼ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    print("=" * 60)
    print("ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘")
    print("=" * 60)
    
    # 1. ì§€ì›ìë³„ ë¬¸ì„œ ë¡œë“œ
    print("\n1ï¸âƒ£ ë¬¸ì„œ ë¡œë“œ ì¤‘...")
    docs_by_candidate = load_documents_from_paths(file_paths, original_filenames)
    
    if not docs_by_candidate:
        raise ValueError("ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    total_docs = sum(len(docs) for docs in docs_by_candidate.values())
    print(f"âœ… ì´ {total_docs} í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ ({len(docs_by_candidate)}ëª…ì˜ ì§€ì›ì)\n")
    
    # 2. ëª¨ë“  ë¬¸ì„œë¥¼ í•˜ë‚˜ë¡œ í•©ì³ì„œ ì²˜ë¦¬
    total_chunks = 0
    processed_candidates = []
    all_chunks = []
    
    for candidate_name, docs in docs_by_candidate.items():
        print(f"\nğŸ”¹ ì§€ì›ì: {candidate_name}")
        
        # ë¬¸ì„œ ë¶„í• 
        print("  2ï¸âƒ£ ë¬¸ì„œ ë¶„í•  ì¤‘...")
        chunks = split_documents(docs)
        print(f"  âœ… {len(chunks)}ê°œì˜ ì²­í¬ ìƒì„± ì™„ë£Œ")
        
        all_chunks.extend(chunks)
        total_chunks += len(chunks)
        processed_candidates.append(candidate_name)
    
    # 3. ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ë˜ëŠ” ì—…ë°ì´íŠ¸ (ë‹¨ì¼ DB)
    print("\n3ï¸âƒ£ ë²¡í„° ìŠ¤í† ì–´ ì²˜ë¦¬ ì¤‘...")
    chroma_dir = get_chroma_dir()
    
    if os.path.exists(chroma_dir) and os.listdir(chroma_dir):
        print(f"  ê¸°ì¡´ ë²¡í„° DBì— ì¶”ê°€í•©ë‹ˆë‹¤...")
        vectorstore = add_to_existing_vectorstore(all_chunks)
    else:
        print(f"  ìƒˆë¡œìš´ ë²¡í„° DBë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
        vectorstore = create_vectorstore(all_chunks)
    
    print(f"  âœ… ë²¡í„° DB ì €ì¥ ì™„ë£Œ: {chroma_dir}")

    # 4. ì§€ì›ì ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ (ë©”íƒ€ë°ì´í„° ê¸°ë°˜)
    print("\n4ï¸âƒ£ ì§€ì›ì ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì¤‘...")
    try:
        candidates_collection = get_candidates_collection()
        unique_candidates = sorted(set(processed_candidates))
        if unique_candidates:
            candidates_collection.upsert(
                ids=unique_candidates,
                metadatas=[{"candidate": name} for name in unique_candidates],
                documents=["" for _ in unique_candidates],
            )
        print("  âœ… ì§€ì›ì ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
    except Exception as e:
        print(f"  âš ï¸ ì§€ì›ì ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    print("\n" + "=" * 60)
    print("âœ¨ ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"ì²˜ë¦¬ëœ ì§€ì›ì: {', '.join(processed_candidates)}")
    print("=" * 60)
    
    return {
        "num_docs": total_docs,
        "num_chunks": total_chunks,
        "candidates": processed_candidates,
        "vectorstore_path": CHROMA_BASE_DIR
    }
